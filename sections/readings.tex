\lesson{}{Recommended Readings}

\subsection{Mathematics}
\label{sub_sec:diffeq}
\textbf{\href{https://a.co/d/f4xltYH}{Finite Difference Methods for Ordinary and Partial Differential Equations: Steady-State and Time-Dependent Problems}}

This book introduces finite difference methods for both ordinary differential equations (ODEs) and partial differential equations (PDEs) and discusses the similarities and differences between algorithm design and stability analysis for different types of equations. A unified view of stability theory for ODEs and PDEs is presented, and the interplay between ODE and PDE analysis is stressed. The text emphasizes standard classical methods, but several newer approaches also are introduced and are described in the context of simple motivating examples. Exercises and student projects are available on the book's webpage, along with Matlab mfiles for implementing methods. Readers will gain an understanding of the essential ideas that underlie the development, analysis, and practical use of finite difference methods as well as the key concepts of stability theory, their relation to one another, and their practical implications. The author provides a foundation from which students can approach more advanced topics.

\subsection{Optimization}
\label{sub_sec:optimization}

\textbf{\href{https://a.co/d/0vaEILG}{Optimization Models}:} 

Emphasizing practical understanding over the technicalities of specific algorithms, this elegant textbook is an accessible introduction to the field of optimization, focusing on powerful and reliable convex optimization techniques. Students and practitioners will learn how to recognize, simplify, model and solve optimization problems - and apply these principles to their own projects. A clear and self-contained introduction to linear algebra demonstrates core mathematical concepts in a way that is easy to follow, and helps students to understand their practical relevance. Requiring only a basic understanding of geometry, calculus, probability and statistics, and striking a careful balance between accessibility and rigor, it enables students to quickly understand the material, without being overwhelmed by complex mathematics. Accompanied by numerous end-of-chapter problems, an online solutions manual for instructors, and relevant examples from diverse fields including engineering, data science, economics, finance, and management, this is the perfect introduction to optimization for undergraduate and graduate students.

\textbf{\href{https://a.co/d/aGdrLVM}{Convex Optimization}}

Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.

\textbf{\href{https://a.co/d/e8KNurs}{Algorithms for Optimization}}

This book offers a comprehensive introduction to optimization with a focus on practical algorithms. The book approaches optimization from an engineering perspective, where the objective is to design a system that optimizes a set of metrics subject to constraints. Readers will learn about computational approaches for a range of challenges, including searching high-dimensional spaces, handling problems where there are multiple competing objectives, and accommodating uncertainty in the metrics. Figures, examples, and exercises convey the intuition behind the mathematical approaches. The text provides concrete implementations in the Julia programming language.


\subsection{Machine Learning and Data Science}
\label{sub_sec:ML_DS_Books}

\textbf{\href{https://a.co/d/aCCUuaQ}{Gaussian Processes for Machine Learning}}

Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.
\newpage